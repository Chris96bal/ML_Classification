{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a0d1b89",
   "metadata": {},
   "source": [
    "# **Overview:**\n",
    "In this section, we will try processing the two categorical columns of our dataset, which are **\"title\"** and **\"artist\"**. More specifically, we will try converting these two columns to numerical representation to be able to \"feed\" them to a Classification model.\n",
    "\n",
    "This conversion will be performed by employing **One-Hot-Encoded vectors**. One-hot encoding is a technique used in machine learning to convert categorical variables into a binary representation, where each unique category becomes a separate binary feature, with a value of 1 indicating the presence of that category and 0 indicating its absence.\n",
    "\n",
    "Goal of this analysis is to evaluate the new models' performance and compare it with that of the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c0a1bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, StratifiedKFold, GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51efbbb",
   "metadata": {},
   "source": [
    "Columns of **\"title\"** and **\"artist\"** will be examined, to check if they are valuable for model training. If they are, we will perform **One-Hot-Encoding** on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "922c73c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>year</th>\n",
       "      <th>bpm</th>\n",
       "      <th>nrgy</th>\n",
       "      <th>dnce</th>\n",
       "      <th>dB</th>\n",
       "      <th>live</th>\n",
       "      <th>val</th>\n",
       "      <th>dur</th>\n",
       "      <th>acous</th>\n",
       "      <th>spch</th>\n",
       "      <th>pop</th>\n",
       "      <th>top genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My Happiness</td>\n",
       "      <td>Connie Francis</td>\n",
       "      <td>1996</td>\n",
       "      <td>107</td>\n",
       "      <td>31</td>\n",
       "      <td>45</td>\n",
       "      <td>-8</td>\n",
       "      <td>13</td>\n",
       "      <td>28</td>\n",
       "      <td>150</td>\n",
       "      <td>75</td>\n",
       "      <td>3</td>\n",
       "      <td>44</td>\n",
       "      <td>adult standards</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How Deep Is Your Love</td>\n",
       "      <td>Bee Gees</td>\n",
       "      <td>1979</td>\n",
       "      <td>105</td>\n",
       "      <td>36</td>\n",
       "      <td>63</td>\n",
       "      <td>-9</td>\n",
       "      <td>13</td>\n",
       "      <td>67</td>\n",
       "      <td>245</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>77</td>\n",
       "      <td>adult standards</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Woman in Love</td>\n",
       "      <td>Barbra Streisand</td>\n",
       "      <td>1980</td>\n",
       "      <td>170</td>\n",
       "      <td>28</td>\n",
       "      <td>47</td>\n",
       "      <td>-16</td>\n",
       "      <td>13</td>\n",
       "      <td>33</td>\n",
       "      <td>232</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>67</td>\n",
       "      <td>adult standards</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Goodbye Yellow Brick Road - Remastered 2014</td>\n",
       "      <td>Elton John</td>\n",
       "      <td>1973</td>\n",
       "      <td>121</td>\n",
       "      <td>47</td>\n",
       "      <td>56</td>\n",
       "      <td>-8</td>\n",
       "      <td>15</td>\n",
       "      <td>40</td>\n",
       "      <td>193</td>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "      <td>63</td>\n",
       "      <td>glam rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Grenade</td>\n",
       "      <td>Bruno Mars</td>\n",
       "      <td>2010</td>\n",
       "      <td>110</td>\n",
       "      <td>56</td>\n",
       "      <td>71</td>\n",
       "      <td>-7</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>223</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>74</td>\n",
       "      <td>pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>But Not For Me</td>\n",
       "      <td>Ella Fitzgerald</td>\n",
       "      <td>1959</td>\n",
       "      <td>80</td>\n",
       "      <td>22</td>\n",
       "      <td>18</td>\n",
       "      <td>-17</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>214</td>\n",
       "      <td>92</td>\n",
       "      <td>4</td>\n",
       "      <td>45</td>\n",
       "      <td>adult standards</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>Surf City</td>\n",
       "      <td>Jan &amp; Dean</td>\n",
       "      <td>2010</td>\n",
       "      <td>148</td>\n",
       "      <td>81</td>\n",
       "      <td>53</td>\n",
       "      <td>-13</td>\n",
       "      <td>23</td>\n",
       "      <td>96</td>\n",
       "      <td>147</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>brill building pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>Dilemma</td>\n",
       "      <td>Nelly</td>\n",
       "      <td>2002</td>\n",
       "      <td>168</td>\n",
       "      <td>55</td>\n",
       "      <td>73</td>\n",
       "      <td>-8</td>\n",
       "      <td>20</td>\n",
       "      <td>61</td>\n",
       "      <td>289</td>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "      <td>77</td>\n",
       "      <td>dance pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>It's Gonna Be Me</td>\n",
       "      <td>*NSYNC</td>\n",
       "      <td>2000</td>\n",
       "      <td>165</td>\n",
       "      <td>87</td>\n",
       "      <td>64</td>\n",
       "      <td>-5</td>\n",
       "      <td>6</td>\n",
       "      <td>88</td>\n",
       "      <td>191</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>62</td>\n",
       "      <td>boy band</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>In The Army Now</td>\n",
       "      <td>Status Quo</td>\n",
       "      <td>2002</td>\n",
       "      <td>105</td>\n",
       "      <td>73</td>\n",
       "      <td>68</td>\n",
       "      <td>-8</td>\n",
       "      <td>14</td>\n",
       "      <td>94</td>\n",
       "      <td>281</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>59</td>\n",
       "      <td>album rock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>438 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           title            artist  year  bpm  \\\n",
       "0                                   My Happiness    Connie Francis  1996  107   \n",
       "1                          How Deep Is Your Love          Bee Gees  1979  105   \n",
       "2                                  Woman in Love  Barbra Streisand  1980  170   \n",
       "3    Goodbye Yellow Brick Road - Remastered 2014        Elton John  1973  121   \n",
       "4                                        Grenade        Bruno Mars  2010  110   \n",
       "..                                           ...               ...   ...  ...   \n",
       "433                               But Not For Me   Ella Fitzgerald  1959   80   \n",
       "434                                    Surf City        Jan & Dean  2010  148   \n",
       "435                                      Dilemma             Nelly  2002  168   \n",
       "436                             It's Gonna Be Me            *NSYNC  2000  165   \n",
       "437                              In The Army Now        Status Quo  2002  105   \n",
       "\n",
       "     nrgy  dnce  dB  live  val  dur  acous  spch  pop           top genre  \n",
       "0      31    45  -8    13   28  150     75     3   44     adult standards  \n",
       "1      36    63  -9    13   67  245     11     3   77     adult standards  \n",
       "2      28    47 -16    13   33  232     25     3   67     adult standards  \n",
       "3      47    56  -8    15   40  193     45     3   63           glam rock  \n",
       "4      56    71  -7    12   23  223     15     6   74                 pop  \n",
       "..    ...   ...  ..   ...  ...  ...    ...   ...  ...                 ...  \n",
       "433    22    18 -17    10   16  214     92     4   45     adult standards  \n",
       "434    81    53 -13    23   96  147     50     3   50  brill building pop  \n",
       "435    55    73  -8    20   61  289     23    14   77           dance pop  \n",
       "436    87    64  -5     6   88  191      5     8   62            boy band  \n",
       "437    73    68  -8    14   94  281     11     2   59          album rock  \n",
       "\n",
       "[438 rows x 14 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"CS98XClassificationTrain.csv\")\n",
    "train = train.drop(['Id'],axis=1)\n",
    "train = train.dropna()\n",
    "train = train.reset_index()\n",
    "train = train.drop([\"index\"], axis = 1)\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a9e580",
   "metadata": {},
   "source": [
    "We will now examine how many unique different elements does each of these columns possess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b5dec81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique song titles in the training set are : 436\n",
      "The number of unique song artists in the training set are : 331\n"
     ]
    }
   ],
   "source": [
    "unique_title = len(train['title'].unique())\n",
    "unique_artist = len(train['artist'].unique())\n",
    "print(\"The number of unique song titles in the training set are :\", unique_title)\n",
    "print(\"The number of unique song artists in the training set are :\", unique_artist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f619d119",
   "metadata": {},
   "source": [
    "Given the large number of unique **\"titles\"** and unique **\"artists\"**, one-hot encoding these variables might lead to a very high-dimensional dataset, which could pose challenges for some algorithms and may lead to the curse of dimensionality.\n",
    "\n",
    "However, since unique **\"artists\"** are less that unique **\"titles\"**, we will experiment with **One-Hot-Encoding** only on the **\"artist\"** column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d0cdb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we will process the train/test datasets to both have 11 numerical variables + the \"artist\" column:\n",
    "# (We will also load the validation set)\n",
    "train = train.drop(['title'], axis = 1)\n",
    "\n",
    "test = pd.read_csv(\"CS98XClassificationTest.csv\")\n",
    "test = test.drop(['Id','title'], axis = 1)\n",
    "test = test.dropna()\n",
    "test = test.reset_index()\n",
    "test = test.drop([\"index\"], axis = 1)\n",
    "\n",
    "test2 = pd.read_csv('CS98XRegressionTest.csv')\n",
    "test2 = test2.dropna()\n",
    "validation_test = test2[\"top genre\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b97198",
   "metadata": {},
   "source": [
    "To use our Classification model effectively, both train and test sets need to have the same dimensions (number of columns). As such, we will firstly concatenate both our train/test sets vertically. After sucessfully creating One-Hot-Encoded vectors, we will split them again into their original size and drop the original **\"artist\"** column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5607831a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly, we concatenate both train and test sets to include all 331 unique artists\n",
    "train_no_genre = train.drop([\"top genre\"], axis = 1)\n",
    "concatenated_train_test = pd.concat([train_no_genre, test], axis=0)\n",
    "\n",
    "# Convert categorical data to one-hot encoded vectors\n",
    "train_categorical = pd.get_dummies(concatenated_train_test['artist'])\n",
    "concatenated_train_test = concatenated_train_test.drop(['artist'],axis=1)\n",
    "\n",
    "# Concatenate the two DataFrames vertically\n",
    "concatenated_df = pd.concat([concatenated_train_test, train_categorical], axis=1)\n",
    "\n",
    "# Split the DataFrame vertically at row 438\n",
    "train_final = concatenated_df.iloc[:438]\n",
    "test_final = concatenated_df.iloc[438:]\n",
    "train_final = pd.concat([train_final, train.loc[:, 'top genre']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e517d1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = train_final.loc[:, 'top genre']\n",
    "X = train_final.drop(['top genre'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5756a1ed",
   "metadata": {},
   "source": [
    "Scaling is generally not necessary for one-hot encoded variables. However, in our case, the combination of one-hot encoded vectors with scaled data produced significantly higher accuracies compared to training with unscaled data.\n",
    "\n",
    "The increase in accuracy ranged from **1%** (**Naive Bayes model**) to **16%** (**Logistic Regression model** + **Multilayer Perceptron model**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d1796e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_X = scaler.fit_transform(X[['live', 'dur', 'spch', 'year', 'bpm', 'nrgy', 'dnce', 'dB', 'val', 'acous', 'pop']])\n",
    "scaled_test = scaler.fit_transform(test_final[['live', 'dur', 'spch', 'year', 'bpm', 'nrgy', 'dnce', 'dB', 'val', 'acous', 'pop']])\n",
    "X[['live', 'dur', 'spch', 'year', 'bpm', 'nrgy', 'dnce', 'dB', 'val', 'acous', 'pop']] = scaled_X\n",
    "test_final[['live', 'dur', 'spch', 'year', 'bpm', 'nrgy', 'dnce', 'dB', 'val', 'acous', 'pop']] = scaled_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e800976",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1dc01aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf = DecisionTreeClassifier()\n",
    "log_clf = LogisticRegression()\n",
    "ovr_clf = OneVsRestClassifier(log_clf)\n",
    "rnd_clf = RandomForestClassifier()\n",
    "kn_clf = KNeighborsClassifier()\n",
    "svm_clf = SVC()\n",
    "mlp_clf = MLPClassifier()\n",
    "gnb_clf = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "679c6298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression : 0.3522727272727273\n",
      "RandomForestClassifier : 0.38636363636363635\n",
      "SVC : 0.375\n",
      "KNeighborsClassifier : 0.26136363636363635\n",
      "MLPClassifier : 0.38636363636363635\n",
      "DecisionTreeClassifier : 0.18181818181818182\n",
      "OneVsRestClassifier : 0.38636363636363635\n",
      "GaussianNB : 0.25\n"
     ]
    }
   ],
   "source": [
    "for clf in (log_clf, rnd_clf, svm_clf, kn_clf, mlp_clf, tree_clf, ovr_clf, gnb_clf):\n",
    "    clf.fit(X_train, Y_train)\n",
    "    ypred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, \":\", accuracy_score(Y_test, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1356e329",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions1 = log_clf.predict(test_final)\n",
    "final_predictions2 = rnd_clf.predict(test_final)\n",
    "final_predictions3 = svm_clf.predict(test_final)\n",
    "final_predictions4 = kn_clf.predict(test_final)\n",
    "final_predictions5 = mlp_clf.predict(test_final)\n",
    "final_predictions6 = tree_clf.predict(test_final)\n",
    "final_predictions7 = ovr_clf.predict(test_final)\n",
    "final_predictions8 = gnb_clf.predict(test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2d06d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Logistic Regression model is: 0.4690265486725664\n",
      "Accuracy of the Random Forest model is: 0.46017699115044247\n",
      "Accuracy of the Support Vector model is: 0.35398230088495575\n",
      "Accuracy of the K-Nearest Neighbors model is: 0.26548672566371684\n",
      "Accuracy of the Multilayer Perceptron model is: 0.5221238938053098\n",
      "Accuracy of the Decision Tree model is: 0.3008849557522124\n",
      "Accuracy of the OneVSRest model is: 0.46017699115044247\n",
      "Accuracy of the Gaussian Naive Bayes model is: 0.35398230088495575\n"
     ]
    }
   ],
   "source": [
    "accuracy1 = accuracy_score(validation_test, final_predictions1)\n",
    "accuracy2 = accuracy_score(validation_test, final_predictions2)\n",
    "accuracy3 = accuracy_score(validation_test, final_predictions3)\n",
    "accuracy4 = accuracy_score(validation_test, final_predictions4)\n",
    "accuracy5 = accuracy_score(validation_test, final_predictions5)\n",
    "accuracy6 = accuracy_score(validation_test, final_predictions6)\n",
    "accuracy7 = accuracy_score(validation_test, final_predictions7)\n",
    "accuracy8 = accuracy_score(validation_test, final_predictions8)\n",
    "\n",
    "print(\"Accuracy of the Logistic Regression model is:\", accuracy1)\n",
    "print(\"Accuracy of the Random Forest model is:\", accuracy2)\n",
    "print(\"Accuracy of the Support Vector model is:\", accuracy3)\n",
    "print(\"Accuracy of the K-Nearest Neighbors model is:\", accuracy4)\n",
    "print(\"Accuracy of the Multilayer Perceptron model is:\", accuracy5)\n",
    "print(\"Accuracy of the Decision Tree model is:\", accuracy6)\n",
    "print(\"Accuracy of the OneVSRest model is:\", accuracy7)\n",
    "print(\"Accuracy of the Gaussian Naive Bayes model is:\", accuracy8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d15585a",
   "metadata": {},
   "source": [
    "One-Hot-Encoding produced the highest results so far, with the **Multilayer Perceptron model** producing **0.5221** on unknown data. Honorable mentions are the **Logistic Regression model** with **0.469** accuracy, the **OneVSRest model** with **0.46** accuracy and the **Random Forest model** with **0.46** accuracy respectively.\n",
    "\n",
    "However, having higher accuracy on the test set rather than on the train set can be a cause of concern. Concatenating train and test set, in the beginning, indicates **Data Leakage** where information from the test set is incorporated into the training process. Furthermore, having small training/testing sets, can create random variations in the data which can lead to fluctuations in performance metrics. In such cases, it's possible for the test set accuracy to be higher by chance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
